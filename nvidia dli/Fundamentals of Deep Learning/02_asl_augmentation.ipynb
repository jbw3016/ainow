{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    MaxPool2D,\n",
    "    Flatten,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    ")\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape : (27455, 785)\n",
      "valid_df.shape : (7172, 785)\n"
     ]
    }
   ],
   "source": [
    "# Load in our data from CSV files\n",
    "train_df = pd.read_csv(r\"C:\\Users\\Harmony25\\Desktop\\LECTURE\\04.Fundamental\\data\\asl_data\\sign_mnist_train.csv\")\n",
    "valid_df = pd.read_csv(r\"C:\\Users\\Harmony25\\Desktop\\LECTURE\\04.Fundamental\\data\\asl_data\\sign_mnist_valid.csv\")\n",
    "\n",
    "print(f'train_df.shape : {train_df.shape}')\n",
    "print(f'valid_df.shape : {valid_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape : (27455, 784)\n",
      "y_train.shape : (27455, 24)\n",
      "x_valid.shape : (7172, 784)\n",
      "y_valid.shape : (7172, 24)\n"
     ]
    }
   ],
   "source": [
    "# Separate out our target values\n",
    "y_train = train_df['label']\n",
    "y_valid = valid_df['label']\n",
    "del train_df['label']\n",
    "del valid_df['label']\n",
    "\n",
    "# Separate our our image vectors\n",
    "x_train = train_df.values\n",
    "x_valid = valid_df.values\n",
    "\n",
    "# Turn our scalar targets into binary categories\n",
    "num_classes = 24\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_valid = keras.utils.to_categorical(y_valid, num_classes)\n",
    "\n",
    "print(f'x_train.shape : {x_train.shape}')\n",
    "print(f'y_train.shape : {y_train.shape}')\n",
    "print(f'x_valid.shape : {x_valid.shape}')\n",
    "print(f'y_valid.shape : {y_valid.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize our image data\n",
    "x_train = x_train / 255\n",
    "x_valid = x_valid / 255\n",
    "\n",
    "# Reshape the image data for the convolutional network\n",
    "x_train = x_train.reshape(-1,28,28,1)\n",
    "x_valid = x_valid.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(75, (3, 3), strides=1, padding=\"same\", activation=\"relu\", \n",
    "                 input_shape=(28, 28, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model.add(Conv2D(50, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model.add(Conv2D(25, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=num_classes, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range=0.1,  # Randomly zoom image\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images horizontally\n",
    "    vertical_flip=False, # Don't randomly flip images vertically\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\Users\\Harmony25\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Harmony25\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "857/857 [==============================] - 25s 28ms/step - loss: 1.0213 - accuracy: 0.6705 - val_loss: 2.0302 - val_accuracy: 0.5551\n",
      "Epoch 2/20\n",
      "857/857 [==============================] - 25s 29ms/step - loss: 0.3035 - accuracy: 0.8956 - val_loss: 0.1949 - val_accuracy: 0.9300\n",
      "Epoch 3/20\n",
      "857/857 [==============================] - 24s 28ms/step - loss: 0.1920 - accuracy: 0.9359 - val_loss: 0.3699 - val_accuracy: 0.8840\n",
      "Epoch 4/20\n",
      "857/857 [==============================] - 24s 29ms/step - loss: 0.1427 - accuracy: 0.9527 - val_loss: 0.8627 - val_accuracy: 0.8081\n",
      "Epoch 5/20\n",
      "857/857 [==============================] - 25s 29ms/step - loss: 0.1260 - accuracy: 0.9594 - val_loss: 0.0604 - val_accuracy: 0.9785\n",
      "Epoch 6/20\n",
      "857/857 [==============================] - 24s 28ms/step - loss: 0.1146 - accuracy: 0.9640 - val_loss: 0.1588 - val_accuracy: 0.9426\n",
      "Epoch 7/20\n",
      "857/857 [==============================] - 23s 27ms/step - loss: 0.0938 - accuracy: 0.9705 - val_loss: 0.1869 - val_accuracy: 0.9413\n",
      "Epoch 8/20\n",
      "857/857 [==============================] - 23s 27ms/step - loss: 0.0861 - accuracy: 0.9726 - val_loss: 0.0989 - val_accuracy: 0.9611\n",
      "Epoch 9/20\n",
      "857/857 [==============================] - 23s 27ms/step - loss: 0.0828 - accuracy: 0.9745 - val_loss: 0.0515 - val_accuracy: 0.9824\n",
      "Epoch 10/20\n",
      "857/857 [==============================] - 24s 29ms/step - loss: 0.0746 - accuracy: 0.9769 - val_loss: 0.1374 - val_accuracy: 0.9568\n",
      "Epoch 11/20\n",
      "857/857 [==============================] - 24s 28ms/step - loss: 0.0668 - accuracy: 0.9787 - val_loss: 0.0265 - val_accuracy: 0.9904\n",
      "Epoch 12/20\n",
      "857/857 [==============================] - 26s 30ms/step - loss: 0.0648 - accuracy: 0.9801 - val_loss: 0.0896 - val_accuracy: 0.9755\n",
      "Epoch 13/20\n",
      "857/857 [==============================] - 27s 32ms/step - loss: 0.0574 - accuracy: 0.9829 - val_loss: 0.0153 - val_accuracy: 0.9968\n",
      "Epoch 14/20\n",
      "857/857 [==============================] - 26s 30ms/step - loss: 0.0591 - accuracy: 0.9831 - val_loss: 0.0369 - val_accuracy: 0.9861\n",
      "Epoch 15/20\n",
      "857/857 [==============================] - 26s 31ms/step - loss: 0.0506 - accuracy: 0.9851 - val_loss: 0.1061 - val_accuracy: 0.9637\n",
      "Epoch 16/20\n",
      "857/857 [==============================] - 26s 30ms/step - loss: 0.0517 - accuracy: 0.9843 - val_loss: 0.0288 - val_accuracy: 0.9937\n",
      "Epoch 17/20\n",
      "857/857 [==============================] - 27s 31ms/step - loss: 0.0477 - accuracy: 0.9852 - val_loss: 0.0528 - val_accuracy: 0.9831\n",
      "Epoch 18/20\n",
      "857/857 [==============================] - 31s 36ms/step - loss: 0.0484 - accuracy: 0.9861 - val_loss: 0.1129 - val_accuracy: 0.9686\n",
      "Epoch 19/20\n",
      "857/857 [==============================] - 26s 31ms/step - loss: 0.0445 - accuracy: 0.9872 - val_loss: 0.0377 - val_accuracy: 0.9837\n",
      "Epoch 20/20\n",
      "857/857 [==============================] - 26s 31ms/step - loss: 0.0443 - accuracy: 0.9876 - val_loss: 0.0323 - val_accuracy: 0.9918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28a1216de10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "img_iter = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "\n",
    "model.fit(img_iter,\n",
    "          epochs=20,\n",
    "          steps_per_epoch=len(x_train)/batch_size, # Run same number of steps we would if we were not using a generator.\n",
    "          validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
