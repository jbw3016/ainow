{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec34iPLEhMFg"
   },
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"../images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTgZfeuKhMFl"
   },
   "source": [
    "# 평가: 저자 판별"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2A9IqGBUhMFl"
   },
   "source": [
    "저작권 표시는 일종의 텍스트 분류 문제입니다.  질병 텍스트 분류 문제에서 그랬던 것처럼 텍스트를 _주제_별로 분류하는 대신 텍스트를 _저자_별로 분류하는 것이 목적입니다.  \n",
    "\n",
    "이와 같은 문제를 해결하려고 시도하는 데 있어서 본질적인 가정은 해당 저자들의 *문체에 몇 가지 차이*가 있으며 *모델이 이러한 차이를 알아차릴 수 있다는 것입니다*.  이것이 BERT 등에도 해당될까요?  언어 모델이 문체를 \"이해\"할 수 있을까요? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbHgjpJDhMFm"
   },
   "source": [
    "### 목차\n",
    "[문제](#문제)<br>\n",
    "[채점](#채점)<br>\n",
    "[1단계: 데이터 준비](#1단계:-데이터-준비)<br>\n",
    "[2단계: 모델 Configuration 준비](#2단계:-모델-Configuration-준비)<br>\n",
    "[3단계: Trainer Configuration 준비](#3단계:-Trainer-Configuration-준비)<br>\n",
    "[4단계: 트레이닝](#4단계:-트레이닝)<br>\n",
    "[5단계: 추론](#5단계:-추론)<br>\n",
    "[6단계: 평가 제출](#6단계:-평가-제출)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8qQODlahMFn"
   },
   "source": [
    "# 문제\n",
    "### 연방주의자 논집 - 역사 미스터리!\n",
    "\n",
    "[연방주의자 논집](https://en.wikipedia.org/wiki/The_Federalist_Papers)은 1787년과 1788년 사이에 [Alexander Hamilton](https://en.wikipedia.org/wiki/Alexander_Hamilton), [James Madison](https://en.wikipedia.org/wiki/James_Madison), [John Jay](https://en.wikipedia.org/wiki/John_Jay)가 작성한 에세이집입니다.  처음에 'Publius'라는 필명으로 발행되었으며 저자들의 의도는 당시 새로운 미국 헌법의 비준을 독려하는 것이었습니다.  후년에 가서 85개 논문의 각 저자가 확인된 목록이 알려졌습니다.  그럼에도 불구하고 이러한 논문의 하위 세트는 저자가 여전히 알려져 있지 않습니다.  연방주의자 논집 저자 판별 문제는 과거에 진행된 대부분의 통계 NLP 연구의 주제였습니다.   이제 자체 BERT 기반 프로젝트 모델을 사용하여 이 문제를 해결해 보겠습니다.\n",
    "<img style=\"float: right;\" src=\"images/HandM.png\" width=400>\n",
    "                                                                                                           \n",
    "구체적으로 말해서, 문제는 논쟁이 되고 있는 각 논문의 저자가 Alexander Hamilton인지, 아니면 James Madison인지 알아내는 것입니다.  이 연습에서는 각 논문의 저자가 한 명이며(즉, *100%* 확실하지는 않지만 공저가 이루어지지 않았음) 각 저자는 확인된 모든 논문 전체에서 보여진 잘 정의된 문체를 지지고 있다고 가정할 수 있습니다. \n",
    "\n",
    "### 프로젝트\n",
    "이 프로젝트를 위해 레이블이 지정된 `train.tsv` 및 `dev.tsv` 데이터세트가 제공됩니다.  논쟁이 되고 있는 각 논문에 하나씩, 총 10개의 테스트 세트가 있습니다.  모든 데이터세트는 `data/federalist_papers_HM` 디렉토리에 포함되어 있습니다.  \n",
    "\n",
    "각 \"문장\"은 사실상 약 256개의 단어로 구성된 문장의 그룹입니다.  레이블은 HAMILTON의 경우 '0', MADISON의 경우 '1'입니다.  예제 파일에는 Madison의 논문보다 Hamilton의 논문이 더 많습니다.  대략적으로 동일한 분포의 레이블 2개를 트레이닝 세트로 사용해 검증 세트를 만들었습니다.\n",
    "\n",
    "여러분이 해야 할 작업은 랩 2에서 했던 것처럼 NeMo를 사용해 뉴럴 네트워크를 구축하는 것입니다.  모델을 트레이닝하고 테스트하게 됩니다.  그런 다음 제공된 대조 코드를 사용해 모델이 이 \"역사 미스터리\"에 제공하는 답변을 확인해 보십시오! \n",
    "\n",
    "이 과정에서 코드 스니펫을 저장한 다음 오토그레이더(autograder)를 사용해 테스트합니다.  이 부분은 노트북 끝의 제출 지침을 따르십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_wFNvpDhMFo"
   },
   "source": [
    "---\n",
    "# 채점\n",
    "최종 결과가 아닌 프로젝트에 맞게 모델을 설정하고 트레이닝하는 능력을 평가받게 됩니다.  이 코딩 평가는 70점 만점이며, 다음과 같이 나뉘어집니다.\n",
    "\n",
    "### 지시문\n",
    "\n",
    "| 단계                                | 평가                                                    | FIXMEs?  | 점수 |\n",
    "|--------------------------------------|-----------------------------------------------------------|----------|--------|\n",
    "| 1. 프로젝트 준비               | 데이터 형식 수정 (정확한 형식)                          |  2       | 10     |\n",
    "| 2. 모델 Configuration 준비   | 재정의를 위한 모델 파라미터 설정                         |  3       | 15     |\n",
    "| 3. Trainer Configuration 준비 | 재정의를 위한 trainer parameters 설정                       |  3       | 15     |\n",
    "| 4. 트레이닝                            | 트레이너 실행(트레이닝 로그가 트레이닝이 올바름을 나타냄) |  4       | 20      |\n",
    "| 5. 추론                             | 추론 실행(결과가 유효한 프로젝트임을 나타냄)          |  0       | 10     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H95O-4nhMFo"
   },
   "source": [
    "이 시점에서 여러분은 도움을 전혀 받지 않고도 프로젝트를 매우 능숙하게 구축할 수 있지만 특정한 변수 이름을 포함해 몇 가지 리소스가 주어집니다.  이는 오토그레이더(autograder)를 위한 것이므로 평가에 이러한 구조를 사용하십시오.  또한, 이번 평가는 커맨트 라인 메소드를 테스트하고 `text_classification_with_bert.py` 스크립트와 커맨드라인 트레이닝 메소드의 사용을 가정하고 있습니다. 모델 이름, 시퀀스 길이, 배치 크기, 학습률, 에포크 수 등과 같은 매개변수를 자유롭게 변경하여 모델을 적합하게 개선할 수 있습니다. \n",
    "\n",
    "신뢰할 수 있는 모델을 구축했다는 확신이 들면 노트북 끝의 제출 지침을 따르십시오.\n",
    "\n",
    "### 리소스 및 힌트\n",
    "* **예제 코드:**<br>\n",
    "왼쪽의 파일 탐색기에서 `lab2_reference_notebooks` 디렉토리를 찾습니다.  여기에는 예제로 사용할 NER 및 텍스트 분류를 위한 랩 2의 솔루션 노트북이 포함되어 있습니다.\n",
    "* **언어 모델 (PRETRAINED_MODEL_NAME):**<br>\n",
    "문체를 더 잘 구분하려면 다양한 언어 모델을 시도하는 것이 도움이 된다는 사실을 알게 될 수 있습니다.  구체적으로 말해서, 대문자 사용이 중요할 수 있으며 이는 \"대소문자를 구분하는\" 모델을 시도해 볼 수 있음을 의미합니다.\n",
    "* **최대 시퀀스 길이 (MAX_SEQ_LEN):**<br>\n",
    "MAX_SEQ_LENGTH에 사용할 수 있는 값은 64, 128 또는 256입니다.  더 큰 모델(BERT-large, Megatron)에는 메모리 부족 오류를 피하기 위해 더 작은 MAX_SEQ_LENGTH가 필요할 수 있습니다.\n",
    "* **클래스 수 (NUM_CLASSES):**<br>\n",
    "연방주의자 논집의 경우 우리는 Hamilton과 Madison에만 관심을 가졌습니다.  John Jay가 작성한 논문은 데이터세트에서 제외되었습니다.\n",
    "* **배치 크기 (BATCH_SIZE):**<br>\n",
    "배치 크기가 크면 더 빨리 트레이닝할 수 있지만 대규모 언어 모델은 가용 메모리를 빠르게 소모합니다.\n",
    "* **메모리 사용량:**<br>\n",
    "일부 모델은 규모가 매우 큽니다.   트레이닝 도중 \"런타임 오류: CUDA 메모리 부족\"이라는 메시지가 수신되면 배치 크기 또는 시퀀스 길이를 줄이거나 더 작은 언어 모델을 선택하고 커널을 다시 시작한 다음 노트북의 처음부터 다시 시도하십시오.\n",
    "* **정확도 및 손실:**<br>\n",
    "이 프로젝트에서는 분명 95% 이상의 정확도를 달성할 수 있습니다.  모델을 트레이닝할 때 정확도 변화 외에도 손실 값에 주의를 기울이십시오.  손실 값을 매우 작게 줄여서 최상의 결과를 얻을 수 있습니다.\n",
    "* **에포크 수(NUM_EPOCHS):**<br>\n",
    "모델을 위해 더 많은 에포크를 실행해야 할 수도 있습니다(또는 아닐 수도 있음!).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmgJ1YAahMFq"
   },
   "source": [
    "---\n",
    "# 1단계: 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "O6FF9nwGhMFr"
   },
   "outputs": [],
   "source": [
    "# Import useful utilities for grading\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def get_latest_model():  \n",
    "    nemo_model_paths = glob.glob('nemo_experiments/TextClassification/*/checkpoints/*.nemo')\n",
    "    # Sort newest first\n",
    "    nemo_model_paths.sort(reverse=True)\n",
    "    return nemo_model_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNcjqbGYhMFs"
   },
   "source": [
    "데이터는 데이터 디렉토리에 위치하고 있습니다. 다음 셀에서 표시하는 목록을 참고하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rHFJfJSUhMFt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.tsv   test49.tsv  test51.tsv  test53.tsv  test55.tsv  test57.tsv  train.tsv\n",
      "test.tsv  test50.tsv  test52.tsv  test54.tsv  test56.tsv  test62.tsv\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '/dli/task/data/federalist_papers_HM'\n",
    "!ls $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_JFNwYThMFu"
   },
   "source": [
    "## 데이터 형식 (평가됨)\n",
    "데이터는 NeMo 텍스트 분류에 올바른 형식이 아닙니다. 데이터를 수정하고 DATA_DIR 에 새로운 데이터 세트를 `train_nemo_format.tsv`및 `dev_nemo_format.tsv`로 저장합니다.  테스트 파일을 사용할 필요가 없습니다.  \n",
    "\n",
    "<i><strong style=\"color:green;\">#FIXME</strong></i> 라인을 완성하고 저장된 셀을 실행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OY79IhPbhMFu"
   },
   "outputs": [],
   "source": [
    "# Correct the format for train.tsv and dev.tsv\n",
    "#   and save the updates in train_nemo_format.tsv and dev_nemo_format.tsv\n",
    "\n",
    "#FIXME train.tsv format\n",
    "#FIXME dev.tsv format\n",
    "\n",
    "!sed 1d $DATA_DIR/train.tsv > $DATA_DIR/train_nemo_format.tsv\n",
    "!sed 1d $DATA_DIR/dev.tsv > $DATA_DIR/dev_nemo_format.tsv\n",
    "!sed 1d $DATA_DIR/test.tsv > $DATA_DIR/test_nemo_format.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8wjEE5HthMFv",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n",
      "train_nemo_format.tsv sample\n",
      "*****\n",
      "Concerning Dangers from Dissensions Between the States For the Independent Journal .To the People of the State of New York : THE three last numbers of this paper have been dedicated to an enumeration of the dangers to which we should be exposed , in a state of disunion , from the arms and arts of foreign nations .I shall now proceed to delineate dangers of a different and , perhaps , still more alarming kind -- those which will in all probability flow from dissensions between the States themselves , and from domestic factions and convulsions .These have been already in some instances slightly anticipated ; but they deserve a more particular and more full investigation .A man must be far gone in Utopian speculations who can seriously doubt that , if these States should either be wholly disunited , or only united in partial confederacies , the subdivisions into which they might be thrown would have frequent and violent contests with each other .To presume a want of motives for such contests as an argument against their existence , would be to forget that men are ambitious , vindictive , and rapacious .To look for a continuation of harmony between a number of independent , unconnected sovereignties in the same neighborhood , would be to disregard the uniform course of human events , and to set at defiance the accumulated experience of ages .The causes of hostility among nations are innumerable .\t0\n",
      "There are some which have a general and almost constant operation upon the collective bodies of society .Of this description are the love of power or the desire of pre-eminence and dominion -- the jealousy of power , or the desire of equality and safety .There are others which have a more circumscribed though an equally operative influence within their spheres .Such are the rivalships and competitions of commerce between commercial nations .And there are others , not less numerous than either of the former , which take their origin entirely in private passions ; in the attachments , enmities , interests , hopes , and fears of leading individuals in the communities of which they are members .Men of this class , whether the favorites of a king or of a people , have in too many instances abused the confidence they possessed ; and assuming the pretext of some public motive , have not scrupled to sacrifice the national tranquillity to personal advantage or personal gratification .The celebrated Pericles , in compliance with the resentment of a prostitute,1 at the expense of much of the blood and treasure of his countrymen , attacked , vanquished , and destroyed the city of the SAMNIANS .The same man , stimulated by private pique against the MEGARENSIANS,2 another nation of Greece , or to avoid a prosecution with which he was threatened as an accomplice of a supposed theft of the statuary Phidias,3 or to get rid of the accusations prepared to be brought against him for dissipating the funds of the state in the purchase of popularity,4 or from a combination of all these causes , was the primitive author of that famous and fatal war , distinguished in the Grecian annals by the name of the PELOPONNESIAN war ; which , after various vicissitudes , intermissions , and renewals , terminated in the ruin of the Athenian commonwealth .\t0\n",
      "The ambitious cardinal , who was prime minister to Henry VIII. , permitting his vanity to aspire to the triple crown,5 entertained hopes of succeeding in the acquisition of that splendid prize by the influence of the Emperor Charles V. To secure the favor and interest of this enterprising and powerful monarch , he precipitated England into a war with France , contrary to the plainest dictates of policy , and at the hazard of the safety and independence , as well of the kingdom over which he presided by his counsels , as of Europe in general .For if there ever was a sovereign who bid fair to realize the project of universal monarchy , it was the Emperor Charles V. , of whose intrigues Wolsey was at once the instrument and the dupe .The influence which the bigotry of one female,6 the petulance of another,7 and the cabals of a third,8 had in the contemporary policy , ferments , and pacifications , of a considerable part of Europe , are topics that have been too often descanted upon not to be generally known .To multiply examples of the agency of personal considerations in the production of great national events , either foreign or domestic , according to their direction , would be an unnecessary waste of time .Those who have but a superficial acquaintance with the sources from which they are to be drawn , will themselves recollect a variety of instances ; and those who have a tolerable knowledge of human nature will not stand in need of such lights to form their opinion either of the reality or extent of that agency .\t0\n",
      "\n",
      "\n",
      "*****\n",
      "dev_nemo_format.tsv sample\n",
      "*****\n",
      "There have been , if I may so express it , almost as many popular as royal wars .The cries of the nation and the importunities of their representatives have , upon various occasions , dragged their monarchs into war , or continued them in it , contrary to their inclinations , and sometimes contrary to the real interests of the State .In that memorable struggle for superiority between the rival houses of AUSTRIA and BOURBON , which so long kept Europe in a flame , it is well known that the antipathies of the English against the French , seconding the ambition , or rather the avarice , of a favorite leader,10 protracted the war beyond the limits marked out by sound policy , and for a considerable time in opposition to the views of the court .The wars of these two last-mentioned nations have in a great measure grown out of commercial considerations , -- the desire of supplanting and the fear of being supplanted , either in particular branches of traffic or in the general advantages of trade and navigation .From this summary of what has taken place in other countries , whose situations have borne the nearest resemblance to our own , what reason can we have to confide in those reveries which would seduce us into an expectation of peace and cordiality between the members of the present confederacy , in a state of separation ? Have we not already seen enough of the fallacy and extravagance of those idle theories which have amused us with promises of an exemption from the imperfections , weaknesses and evils incident to society in every shape ?\t0\n",
      "They would , at the same time , be necessitated to strengthen the executive arm of government , in doing which their constitutions would acquire a progressive direction toward monarchy .It is of the nature of war to increase the executive at the expense of the legislative authority .The expedients which have been mentioned would soon give the States or confederacies that made use of them a superiority over their neighbors .Small states , or states of less natural strength , under vigorous governments , and with the assistance of disciplined armies , have often triumphed over large states , or states of greater natural strength , which have been destitute of these advantages .Neither the pride nor the safety of the more important States or confederacies would permit them long to submit to this mortifying and adventitious superiority .They would quickly resort to means similar to those by which it had been effected , to reinstate themselves in their lost pre-eminence .Thus , we should , in a little time , see established in every part of this country the same engines of despotism which have been the scourge of the Old World .This , at least , would be the natural course of things ; and our reasonings will be the more likely to be just , in proportion as they are accommodated to this standard .These are not vague inferences drawn from supposed or speculative defects in a Constitution , the whole power of which is lodged in the hands of a people , or their representatives and delegates , but they are solid conclusions , drawn from the natural and necessary progress of human affairs .\t0\n",
      "Such a point gained from the British government , and which could not be expected without an equivalent in exemptions and immunities in our markets , would be likely to have a correspondent effect on the conduct of other nations , who would not be inclined to see themselves altogether supplanted in our trade .A further resource for influencing the conduct of European nations toward us , in this respect , would arise from the establishment of a federal navy .There can be no doubt that the continuance of the Union under an efficient government would put it in our power , at a period not very distant , to create a navy which , if it could not vie with those of the great maritime powers , would at least be of respectable weight if thrown into the scale of either of two contending parties .This would be more peculiarly the case in relation to operations in the West Indies .A few ships of the line , sent opportunely to the reinforcement of either side , would often be sufficient to decide the fate of a campaign , on the event of which interests of the greatest magnitude were suspended .Our position is , in this respect , a most commanding one .And if to this consideration we add that of the usefulness of supplies from this country , in the prosecution of military operations in the West Indies , it will readily be perceived that a situation so favorable would enable us to bargain with great advantage for commercial privileges .\t0\n"
     ]
    }
   ],
   "source": [
    "# check your work\n",
    "print(\"*****\\ntrain_nemo_format.tsv sample\\n*****\")\n",
    "!head -n 3 $DATA_DIR/train_nemo_format.tsv\n",
    "print(\"\\n\\n*****\\ndev_nemo_format.tsv sample\\n*****\")\n",
    "!head -n 3 $DATA_DIR/dev_nemo_format.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dnU-9XBthMFv"
   },
   "outputs": [],
   "source": [
    "# Run to save for assessment- DO NOT CHANGE\n",
    "import os.path\n",
    "DATA_DIR = '/dli/task/data/federalist_papers_HM'\n",
    "step1 = []\n",
    "try:\n",
    "    with open(os.path.join(DATA_DIR,'train_nemo_format.tsv')) as f:\n",
    "        content = f.readlines()\n",
    "        step1 += content[:2]\n",
    "    with open(os.path.join(DATA_DIR,'dev_nemo_format.tsv')) as f:\n",
    "        content = f.readlines()\n",
    "        step1 += content[:2]\n",
    "except:\n",
    "    pass\n",
    "                \n",
    "with open(\"my_assessment/step1.json\", \"w\") as outfile: \n",
    "    json.dump(step1, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEZJBUGGhMFw"
   },
   "source": [
    "---\n",
    "# 2단계: 모델 Configuration 준비\n",
    "기본 모델 configuration 과 가용한 언어 모델을 검토합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lMdUdb2shMFw",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nemo_path: text_classification_model.nemo\n",
      "tokenizer:\n",
      "  tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "  vocab_file: null\n",
      "  tokenizer_model: null\n",
      "  special_tokens: null\n",
      "language_model:\n",
      "  pretrained_model_name: bert-base-uncased\n",
      "  lm_checkpoint: null\n",
      "  config_file: null\n",
      "  config: null\n",
      "classifier_head:\n",
      "  num_output_layers: 2\n",
      "  fc_dropout: 0.1\n",
      "class_labels:\n",
      "  class_labels_file: null\n",
      "dataset:\n",
      "  num_classes: ???\n",
      "  do_lower_case: false\n",
      "  max_seq_length: 256\n",
      "  class_balancing: null\n",
      "  use_cache: false\n",
      "train_ds:\n",
      "  file_path: null\n",
      "  batch_size: 64\n",
      "  shuffle: true\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "validation_ds:\n",
      "  file_path: null\n",
      "  batch_size: 64\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "test_ds:\n",
      "  file_path: null\n",
      "  batch_size: 64\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "optim:\n",
      "  name: adam\n",
      "  lr: 2.0e-05\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.999\n",
      "  weight_decay: 0.01\n",
      "  sched:\n",
      "    name: WarmupAnnealing\n",
      "    warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    monitor: val_loss\n",
      "    reduce_on_plateau: false\n",
      "infer_samples:\n",
      "- by the end of no such thing the audience , like beatrice , has a watchful affection\n",
      "  for the monster .\n",
      "- director rob marshall went out gunning to make a great one .\n",
      "- uneasy mishmash of styles and genres .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the default model portion of the config file\n",
    "CONFIG_DIR = \"/dli/task/nemo/examples/nlp/text_classification/conf\"\n",
    "CONFIG_FILE = \"text_classification_config.yaml\"\n",
    "\n",
    "config = OmegaConf.load(CONFIG_DIR + \"/\" + CONFIG_FILE)\n",
    "print(OmegaConf.to_yaml(config.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ztWDlhk_hMFx",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['megatron-bert-345m-uncased',\n",
       " 'megatron-bert-345m-cased',\n",
       " 'megatron-bert-uncased',\n",
       " 'megatron-bert-cased',\n",
       " 'biomegatron-bert-345m-uncased',\n",
       " 'biomegatron-bert-345m-cased',\n",
       " 'bert-base-uncased',\n",
       " 'bert-large-uncased',\n",
       " 'bert-base-cased',\n",
       " 'bert-large-cased',\n",
       " 'bert-base-multilingual-uncased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'bert-base-chinese',\n",
       " 'bert-base-german-cased',\n",
       " 'bert-large-uncased-whole-word-masking',\n",
       " 'bert-large-cased-whole-word-masking',\n",
       " 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
       " 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
       " 'bert-base-cased-finetuned-mrpc',\n",
       " 'bert-base-german-dbmdz-cased',\n",
       " 'bert-base-german-dbmdz-uncased',\n",
       " 'cl-tohoku/bert-base-japanese',\n",
       " 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
       " 'cl-tohoku/bert-base-japanese-char',\n",
       " 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
       " 'TurkuNLP/bert-base-finnish-cased-v1',\n",
       " 'TurkuNLP/bert-base-finnish-uncased-v1',\n",
       " 'wietsedv/bert-base-dutch-cased',\n",
       " 'distilbert-base-uncased',\n",
       " 'distilbert-base-uncased-distilled-squad',\n",
       " 'distilbert-base-cased',\n",
       " 'distilbert-base-cased-distilled-squad',\n",
       " 'distilbert-base-german-cased',\n",
       " 'distilbert-base-multilingual-cased',\n",
       " 'distilbert-base-uncased-finetuned-sst-2-english',\n",
       " 'roberta-base',\n",
       " 'roberta-large',\n",
       " 'roberta-large-mnli',\n",
       " 'distilroberta-base',\n",
       " 'roberta-base-openai-detector',\n",
       " 'roberta-large-openai-detector',\n",
       " 'albert-base-v1',\n",
       " 'albert-large-v1',\n",
       " 'albert-xlarge-v1',\n",
       " 'albert-xxlarge-v1',\n",
       " 'albert-base-v2',\n",
       " 'albert-large-v2',\n",
       " 'albert-xlarge-v2',\n",
       " 'albert-xxlarge-v2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what BERT-like language models are available\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "nemo_nlp.modules.get_pretrained_lm_models_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7P_unX1hMFy"
   },
   "source": [
    "## 파라미터 설정하기 (평가됨)\n",
    "<i><strong style=\"color:green;\">#FIXME</strong></i> 라인을 완성하고 저장된 셀을 실행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0WE37OZThMFy"
   },
   "outputs": [],
   "source": [
    "# set the values\n",
    "NUM_CLASSES = 2\n",
    "MAX_SEQ_LENGTH = 256\n",
    "BATCH_SIZE = 64\n",
    "PATH_TO_TRAIN_FILE = \"/dli/task/data/federalist_papers_HM/train_nemo_format.tsv\"\n",
    "PATH_TO_VAL_FILE = \"/dli/task/data/federalist_papers_HM/dev_nemo_format.tsv\"\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-uncased' # change as desired\n",
    "LR = 1e-4 # change as desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cfcOUTIyhMFy"
   },
   "outputs": [],
   "source": [
    "# Run to save for assessment- DO NOT CHANGE\n",
    "with open(\"my_assessment/step2.json\", \"w\") as outfile: \n",
    "    json.dump([MAX_SEQ_LENGTH, NUM_CLASSES, BATCH_SIZE], outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKVk38hXhMFz"
   },
   "source": [
    "---\n",
    "# 3단계: Trainer Configuration 준비\n",
    "기본 trainer 와 exp_manager configuration를 검토합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KLESTU57hMFz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpus: 1\n",
      "num_nodes: 1\n",
      "max_epochs: 100\n",
      "max_steps: null\n",
      "accumulate_grad_batches: 1\n",
      "gradient_clip_val: 0.0\n",
      "amp_level: O0\n",
      "precision: 32\n",
      "accelerator: ddp\n",
      "log_every_n_steps: 1\n",
      "val_check_interval: 1.0\n",
      "resume_from_checkpoint: null\n",
      "num_sanity_val_steps: 0\n",
      "checkpoint_callback: false\n",
      "logger: false\n",
      "\n",
      "exp_dir: null\n",
      "name: TextClassification\n",
      "create_tensorboard_logger: true\n",
      "create_checkpoint_callback: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(config.trainer))\n",
    "print(OmegaConf.to_yaml(config.exp_manager))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4tOzKGRhMFz"
   },
   "source": [
    "## 파라미터 지정 (평가됨)\n",
    "자동 혼합 정밀도를 FP16 정밀도로 레벨 1로 설정합니다. MAX_EPOCHS to를 적절한 수준 (약 5~20)으로 설정합니다.  <br><i><strong style=\"color:green;\">#FIXME</strong></i> 라인을 완성하고 저장된 셀을 실행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "5oiYHIkXhMF0"
   },
   "outputs": [],
   "source": [
    "# set the values\n",
    "MAX_EPOCHS = 10\n",
    "AMP_LEVEL = 'O1' \n",
    "PRECISION = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "T-lnvk2QhMF0"
   },
   "outputs": [],
   "source": [
    "# Run to save for assessment - DO NOT CHANGE\n",
    "with open(\"my_assessment/step3.json\", \"w\") as outfile: \n",
    "    json.dump([MAX_EPOCHS, AMP_LEVEL, PRECISION], outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCZJT3_IhMF1",
    "tags": []
   },
   "source": [
    "---\n",
    "# 4단계: 트레이닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHahQCm3hMF1"
   },
   "source": [
    "### Trainer 실행하기 (평가됨)\n",
    "<i><strong style=\"color:green;\">#FIXME</strong></i>를 완성하시고 다음 셀에서 트레이닝 및 검증 배치 사이즈, amp 레벨과 정밀도를 확인하세요.  그 다음 트레이닝을 진행 후 저장된 셀을 실행하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "n32XNxN_hMF1",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-06 15:10:27 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/omegaconf/basecontainer.py:225: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "    Use OmegaConf.to_yaml(cfg)\n",
      "    \n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo I 2024-03-06 15:10:27 text_classification_with_bert:110] \n",
      "    Config Params:\n",
      "    trainer:\n",
      "      gpus: 1\n",
      "      num_nodes: 1\n",
      "      max_epochs: 10\n",
      "      max_steps: null\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 0.0\n",
      "      amp_level: O1\n",
      "      precision: 16\n",
      "      accelerator: ddp\n",
      "      log_every_n_steps: 1\n",
      "      val_check_interval: 1.0\n",
      "      resume_from_checkpoint: null\n",
      "      num_sanity_val_steps: 0\n",
      "      checkpoint_callback: false\n",
      "      logger: false\n",
      "    model:\n",
      "      nemo_path: text_classification_model.nemo\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: null\n",
      "        tokenizer_model: null\n",
      "        special_tokens: null\n",
      "      language_model:\n",
      "        pretrained_model_name: bert-base-uncased\n",
      "        lm_checkpoint: null\n",
      "        config_file: null\n",
      "        config: null\n",
      "      classifier_head:\n",
      "        num_output_layers: 2\n",
      "        fc_dropout: 0.1\n",
      "      class_labels:\n",
      "        class_labels_file: null\n",
      "      dataset:\n",
      "        num_classes: 2\n",
      "        do_lower_case: false\n",
      "        max_seq_length: 256\n",
      "        class_balancing: null\n",
      "        use_cache: false\n",
      "      train_ds:\n",
      "        file_path: /dli/task/data/federalist_papers_HM/train_nemo_format.tsv\n",
      "        batch_size: 64\n",
      "        shuffle: true\n",
      "        num_samples: -1\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      validation_ds:\n",
      "        file_path: /dli/task/data/federalist_papers_HM/dev_nemo_format.tsv\n",
      "        batch_size: 64\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      test_ds:\n",
      "        file_path: null\n",
      "        batch_size: 64\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      optim:\n",
      "        name: adam\n",
      "        lr: 0.0001\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        weight_decay: 0.01\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          warmup_steps: null\n",
      "          warmup_ratio: 0.1\n",
      "          last_epoch: -1\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "      infer_samples: []\n",
      "    exp_manager:\n",
      "      exp_dir: null\n",
      "      name: TextClassification\n",
      "      create_tensorboard_logger: true\n",
      "      create_checkpoint_callback: true\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "[NeMo I 2024-03-06 15:10:27 exp_manager:216] Experiments will be logged at /dli/task/nemo_experiments/TextClassification/2024-03-06_15-10-27\n",
      "[NeMo I 2024-03-06 15:10:27 exp_manager:563] TensorboardLogger has been set up\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo I 2024-03-06 15:10:27 text_classification_dataset:120] Read 502 examples from /dli/task/data/federalist_papers_HM/train_nemo_format.tsv.\n",
      "[NeMo I 2024-03-06 15:10:28 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-03-06 15:10:28 text_classification_dataset:239] example 0: ['Or', ',', 'if', 'such', 'a', 'trial', 'of', 'firmness', 'between', 'the', 'two', 'branches', 'were', 'hazarded', ',', 'would', 'not', 'the', 'one', 'be', 'as', 'likely', 'first', 'to', 'yield', 'as', 'the', 'other', '?', 'These', 'questions', 'will', 'create', 'no', 'difficulty', 'with', 'those', 'who', 'reflect', 'that', 'in', 'all', 'cases', 'the', 'smaller', 'the', 'number', ',', 'and', 'the', 'more', 'permanent', 'and', 'conspicuous', 'the', 'station', ',', 'of', 'men', 'in', 'power', ',', 'the', 'stronger', 'must', 'be', 'the', 'interest', 'which', 'they', 'will', 'individually', 'feel', 'in', 'whatever', 'concerns', 'the', 'government', '.Those', 'who', 'represent', 'the', 'dignity', 'of', 'their', 'country', 'in', 'the', 'eyes', 'of', 'other', 'nations', ',', 'will', 'be', 'particularly', 'sensible', 'to', 'every', 'prospect', 'of', 'public', 'danger', ',', 'or', 'of', 'dishonorable', 'stagnation', 'in', 'public', 'affairs', '.To', 'those', 'causes', 'we', 'are', 'to', 'ascribe', 'the', 'continual', 'triumph', 'of', 'the', 'British', 'House', 'of', 'Commons', 'over', 'the', 'other', 'branches', 'of', 'the', 'government', ',', 'whenever', 'the', 'engine', 'of', 'a', 'money', 'bill', 'has', 'been', 'employed', '.An', 'absolute', 'inflexibility', 'on', 'the', 'side', 'of', 'the', 'latter', ',', 'although', 'it', 'could', 'not', 'have', 'failed', 'to', 'involve', 'every', 'department', 'of', 'the', 'state', 'in', 'the', 'general', 'confusion', ',', 'has', 'neither', 'been', 'apprehended', 'nor', 'experienced', '.The', 'utmost', 'degree', 'of', 'firmness', 'that', 'can', 'be', 'displayed', 'by', 'the', 'federal', 'Senate', 'or', 'President', ',', 'will', 'not', 'be', 'more', 'than', 'equal', 'to', 'a', 'resistance', 'in', 'which', 'they', 'will', 'be', 'supported', 'by', 'constitutional', 'and', 'patriotic', 'principles', '.In', 'this', 'review', 'of', 'the', 'Constitution', 'of', 'the', 'House', 'of', 'Representatives', ',', 'I', 'have', 'passed', 'over', 'the', 'circumstances', 'of', 'economy', ',', 'which', ',', 'in', 'the', 'present', 'state', 'of', 'affairs', ',', 'might', 'have', 'had', 'some', 'effect', 'in', 'lessening', 'the', 'temporary', 'number', 'of', 'representatives', ',', 'and', 'a', 'disregard', 'of', 'which', 'would', 'probably', 'have', 'been', 'as', 'rich', 'a', 'theme', 'of', 'declamation', 'against', 'the', 'Constitution', 'as', 'has', 'been', 'shown', 'by', 'the', 'smallness', 'of', 'the', 'number', 'proposed', '.']\n",
      "[NeMo I 2024-03-06 15:10:28 text_classification_dataset:240] subtokens: [CLS] or , if such a trial of firm ##ness between the two branches were hazard ##ed , would not the one be as likely first to yield as the other ? these questions will create no difficulty with those who reflect that in all cases the smaller the number , and the more permanent and conspicuous the station , of men in power , the stronger must be the interest which they will individually feel in whatever concerns the government . those who represent the dignity of their country in the eyes of other nations , will be particularly sensible to every prospect of public danger , or of dish ##ono ##rable st ##ag ##nation in public affairs . to those causes we are to as ##cr ##ibe the continual triumph of the british house of commons over the other branches of the government , whenever the engine of a money bill has been employed . an absolute in ##fle ##xi ##bility on the side of the latter , although it could not have failed to involve every department of the state in the general confusion , has neither been app ##re ##hend ##ed nor experienced . the utmost degree of firm ##ness that can be displayed by the federal senate or president , will not be more than equal to a resistance in which they will be supported by constitutional and patriotic principles . in this review of the constitution of the house of representatives , i have passed over the circumstances of economy [SEP]\n",
      "[NeMo I 2024-03-06 15:10:28 text_classification_dataset:241] input_ids: 101 2030 1010 2065 2107 1037 3979 1997 3813 2791 2090 1996 2048 5628 2020 15559 2098 1010 2052 2025 1996 2028 2022 2004 3497 2034 2000 10750 2004 1996 2060 1029 2122 3980 2097 3443 2053 7669 2007 2216 2040 8339 2008 1999 2035 3572 1996 3760 1996 2193 1010 1998 1996 2062 4568 1998 19194 1996 2276 1010 1997 2273 1999 2373 1010 1996 6428 2442 2022 1996 3037 2029 2027 2097 14258 2514 1999 3649 5936 1996 2231 1012 2216 2040 5050 1996 13372 1997 2037 2406 1999 1996 2159 1997 2060 3741 1010 2097 2022 3391 21082 2000 2296 9824 1997 2270 5473 1010 2030 1997 9841 17175 16670 2358 8490 9323 1999 2270 3821 1012 2000 2216 5320 2057 2024 2000 2004 26775 20755 1996 27222 10911 1997 1996 2329 2160 1997 7674 2058 1996 2060 5628 1997 1996 2231 1010 7188 1996 3194 1997 1037 2769 3021 2038 2042 4846 1012 2019 7619 1999 21031 9048 8553 2006 1996 2217 1997 1996 3732 1010 2348 2009 2071 2025 2031 3478 2000 9125 2296 2533 1997 1996 2110 1999 1996 2236 6724 1010 2038 4445 2042 10439 2890 22342 2098 4496 5281 1012 1996 27917 3014 1997 3813 2791 2008 2064 2022 6913 2011 1996 2976 4001 2030 2343 1010 2097 2025 2022 2062 2084 5020 2000 1037 5012 1999 2029 2027 2097 2022 3569 2011 6543 1998 14314 6481 1012 1999 2023 3319 1997 1996 4552 1997 1996 2160 1997 4505 1010 1045 2031 2979 2058 1996 6214 1997 4610 102\n",
      "[NeMo I 2024-03-06 15:10:28 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-06 15:10:28 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-03-06 15:10:28 text_classification_dataset:244] label: 1\n",
      "[NeMo I 2024-03-06 15:10:28 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-03-06 15:10:28 text_classification_dataset:239] example 1: ['But', 'the', 'mild', 'voice', 'of', 'reason', ',', 'pleading', 'the', 'cause', 'of', 'an', 'enlarged', 'and', 'permanent', 'interest', ',', 'is', 'but', 'too', 'often', 'drowned', ',', 'before', 'public', 'bodies', 'as', 'well', 'as', 'individuals', ',', 'by', 'the', 'clamors', 'of', 'an', 'impatient', 'avidity', 'for', 'immediate', 'and', 'immoderate', 'gain', '.The', 'necessity', 'of', 'a', 'superintending', 'authority', 'over', 'the', 'reciprocal', 'trade', 'of', 'confederated', 'States', ',', 'has', 'been', 'illustrated', 'by', 'other', 'examples', 'as', 'well', 'as', 'our', 'own', '.In', 'Switzerland', ',', 'where', 'the', 'Union', 'is', 'so', 'very', 'slight', ',', 'each', 'canton', 'is', 'obliged', 'to', 'allow', 'to', 'merchandises', 'a', 'passage', 'through', 'its', 'jurisdiction', 'into', 'other', 'cantons', ',', 'without', 'an', 'augmentation', 'of', 'the', 'tolls', '.In', 'Germany', 'it', 'is', 'a', 'law', 'of', 'the', 'empire', ',', 'that', 'the', 'princes', 'and', 'states', 'shall', 'not', 'lay', 'tolls', 'or', 'customs', 'on', 'bridges', ',', 'rivers', ',', 'or', 'passages', ',', 'without', 'the', 'consent', 'of', 'the', 'emperor', 'and', 'the', 'diet', ';', 'though', 'it', 'appears', 'from', 'a', 'quotation', 'in', 'an', 'antecedent', 'paper', ',', 'that', 'the', 'practice', 'in', 'this', ',', 'as', 'in', 'many', 'other', 'instances', 'in', 'that', 'confederacy', ',', 'has', 'not', 'followed', 'the', 'law', ',', 'and', 'has', 'produced', 'there', 'the', 'mischiefs', 'which', 'have', 'been', 'foreseen', 'here', '.Among', 'the', 'restraints', 'imposed', 'by', 'the', 'Union', 'of', 'the', 'Netherlands', 'on', 'its', 'members', ',', 'one', 'is', ',', 'that', 'they', 'shall', 'not', 'establish', 'imposts', 'disadvantageous', 'to', 'their', 'neighbors', ',', 'without', 'the', 'general', 'permission', '.The', 'regulation', 'of', 'commerce', 'with', 'the', 'Indian', 'tribes', 'is', 'very', 'properly', 'unfettered', 'from', 'two', 'limitations', 'in', 'the', 'articles', 'of', 'Confederation', ',', 'which', 'render', 'the', 'provision', 'obscure', 'and', 'contradictory', '.']\n",
      "[NeMo I 2024-03-06 15:10:28 text_classification_dataset:240] subtokens: [CLS] but the mild voice of reason , pleading the cause of an enlarged and permanent interest , is but too often drowned , before public bodies as well as individuals , by the cl ##amo ##rs of an impatient avid ##ity for immediate and im ##mo ##der ##ate gain . the necessity of a super ##int ##ending authority over the reciprocal trade of confederate ##d states , has been illustrated by other examples as well as our own . in switzerland , where the union is so very slight , each canton is obliged to allow to merchandise ##s a passage through its jurisdiction into other canton ##s , without an aug ##ment ##ation of the toll ##s . in germany it is a law of the empire , that the princes and states shall not lay toll ##s or customs on bridges , rivers , or passages , without the consent of the emperor and the diet ; though it appears from a quota ##tion in an ant ##ece ##dent paper , that the practice in this , as in many other instances in that confederacy , has not followed the law , and has produced there the mischief ##s which have been fore ##see ##n here . among the restraints imposed by the union of the netherlands on its members , one is , that they shall not establish imp ##ost ##s disadvantage ##ous to their neighbors , without the general permission . the regulation of commerce with the indian tribes is very [SEP]\n",
      "[NeMo I 2024-03-06 15:10:28 text_classification_dataset:241] input_ids: 101 2021 1996 10256 2376 1997 3114 1010 16418 1996 3426 1997 2019 11792 1998 4568 3037 1010 2003 2021 2205 2411 12805 1010 2077 2270 4230 2004 2092 2004 3633 1010 2011 1996 18856 22591 2869 1997 2019 17380 18568 3012 2005 6234 1998 10047 5302 4063 3686 5114 1012 1996 13185 1997 1037 3565 18447 18537 3691 2058 1996 28309 3119 1997 8055 2094 2163 1010 2038 2042 7203 2011 2060 4973 2004 2092 2004 2256 2219 1012 1999 5288 1010 2073 1996 2586 2003 2061 2200 7263 1010 2169 8770 2003 14723 2000 3499 2000 16359 2015 1037 6019 2083 2049 7360 2046 2060 8770 2015 1010 2302 2019 15476 3672 3370 1997 1996 9565 2015 1012 1999 2762 2009 2003 1037 2375 1997 1996 3400 1010 2008 1996 12000 1998 2163 4618 2025 3913 9565 2015 2030 8205 2006 7346 1010 5485 1010 2030 13768 1010 2302 1996 9619 1997 1996 3750 1998 1996 8738 1025 2295 2009 3544 2013 1037 20563 3508 1999 2019 14405 26005 16454 3259 1010 2008 1996 3218 1999 2023 1010 2004 1999 2116 2060 12107 1999 2008 18179 1010 2038 2025 2628 1996 2375 1010 1998 2038 2550 2045 1996 25166 2015 2029 2031 2042 18921 19763 2078 2182 1012 2426 1996 28054 9770 2011 1996 2586 1997 1996 4549 2006 2049 2372 1010 2028 2003 1010 2008 2027 4618 2025 5323 17727 14122 2015 20502 3560 2000 2037 10638 1010 2302 1996 2236 6656 1012 1996 7816 1997 6236 2007 1996 2796 6946 2003 2200 102\n",
      "[NeMo I 2024-03-06 15:10:28 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-06 15:10:28 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-03-06 15:10:28 text_classification_dataset:244] label: 1\n",
      "[NeMo W 2024-03-06 15:10:36 text_classification_dataset:250] Found 502 out of 502 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo I 2024-03-06 15:10:36 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-03-06 15:10:36 data_preprocessing:301] Min: 257 |                  Max: 257 |                  Mean: 257.0 |                  Median: 257.0\n",
      "[NeMo I 2024-03-06 15:10:36 data_preprocessing:307] 75 percentile: 257.00\n",
      "[NeMo I 2024-03-06 15:10:36 data_preprocessing:308] 99 percentile: 257.00\n",
      "[NeMo I 2024-03-06 15:10:36 text_classification_dataset:120] Read 115 examples from /dli/task/data/federalist_papers_HM/dev_nemo_format.tsv.\n",
      "[NeMo I 2024-03-06 15:10:36 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-03-06 15:10:36 text_classification_dataset:239] example 0: ['There', 'have', 'been', ',', 'if', 'I', 'may', 'so', 'express', 'it', ',', 'almost', 'as', 'many', 'popular', 'as', 'royal', 'wars', '.The', 'cries', 'of', 'the', 'nation', 'and', 'the', 'importunities', 'of', 'their', 'representatives', 'have', ',', 'upon', 'various', 'occasions', ',', 'dragged', 'their', 'monarchs', 'into', 'war', ',', 'or', 'continued', 'them', 'in', 'it', ',', 'contrary', 'to', 'their', 'inclinations', ',', 'and', 'sometimes', 'contrary', 'to', 'the', 'real', 'interests', 'of', 'the', 'State', '.In', 'that', 'memorable', 'struggle', 'for', 'superiority', 'between', 'the', 'rival', 'houses', 'of', 'AUSTRIA', 'and', 'BOURBON', ',', 'which', 'so', 'long', 'kept', 'Europe', 'in', 'a', 'flame', ',', 'it', 'is', 'well', 'known', 'that', 'the', 'antipathies', 'of', 'the', 'English', 'against', 'the', 'French', ',', 'seconding', 'the', 'ambition', ',', 'or', 'rather', 'the', 'avarice', ',', 'of', 'a', 'favorite', 'leader,10', 'protracted', 'the', 'war', 'beyond', 'the', 'limits', 'marked', 'out', 'by', 'sound', 'policy', ',', 'and', 'for', 'a', 'considerable', 'time', 'in', 'opposition', 'to', 'the', 'views', 'of', 'the', 'court', '.The', 'wars', 'of', 'these', 'two', 'last-mentioned', 'nations', 'have', 'in', 'a', 'great', 'measure', 'grown', 'out', 'of', 'commercial', 'considerations', ',', '--', 'the', 'desire', 'of', 'supplanting', 'and', 'the', 'fear', 'of', 'being', 'supplanted', ',', 'either', 'in', 'particular', 'branches', 'of', 'traffic', 'or', 'in', 'the', 'general', 'advantages', 'of', 'trade', 'and', 'navigation', '.From', 'this', 'summary', 'of', 'what', 'has', 'taken', 'place', 'in', 'other', 'countries', ',', 'whose', 'situations', 'have', 'borne', 'the', 'nearest', 'resemblance', 'to', 'our', 'own', ',', 'what', 'reason', 'can', 'we', 'have', 'to', 'confide', 'in', 'those', 'reveries', 'which', 'would', 'seduce', 'us', 'into', 'an', 'expectation', 'of', 'peace', 'and', 'cordiality', 'between', 'the', 'members', 'of', 'the', 'present', 'confederacy', ',', 'in', 'a', 'state', 'of', 'separation', '?', 'Have', 'we', 'not', 'already', 'seen', 'enough', 'of', 'the', 'fallacy', 'and', 'extravagance', 'of', 'those', 'idle', 'theories', 'which', 'have', 'amused', 'us', 'with', 'promises', 'of', 'an', 'exemption', 'from', 'the', 'imperfections', ',', 'weaknesses', 'and', 'evils', 'incident', 'to', 'society', 'in', 'every', 'shape', '?']\n",
      "[NeMo I 2024-03-06 15:10:36 text_classification_dataset:240] subtokens: [CLS] there have been , if i may so express it , almost as many popular as royal wars . the cries of the nation and the import ##uni ##ties of their representatives have , upon various occasions , dragged their monarchs into war , or continued them in it , contrary to their inclination ##s , and sometimes contrary to the real interests of the state . in that memorable struggle for superiority between the rival houses of austria and bourbon , which so long kept europe in a flame , it is well known that the anti ##path ##ies of the english against the french , second ##ing the ambition , or rather the ava ##rice , of a favorite leader , 10 pro ##tracted the war beyond the limits marked out by sound policy , and for a considerable time in opposition to the views of the court . the wars of these two last - mentioned nations have in a great measure grown out of commercial considerations , - - the desire of su ##pp ##lan ##ting and the fear of being su ##pp ##lan ##ted , either in particular branches of traffic or in the general advantages of trade and navigation . from this summary of what has taken place in other countries , whose situations have borne the nearest resemblance to our own , what reason can we have to con ##fide in those rev ##eries which would seduce us into an expectation of peace and cord ##ial ##ity between [SEP]\n",
      "[NeMo I 2024-03-06 15:10:36 text_classification_dataset:241] input_ids: 101 2045 2031 2042 1010 2065 1045 2089 2061 4671 2009 1010 2471 2004 2116 2759 2004 2548 5233 1012 1996 12842 1997 1996 3842 1998 1996 12324 19496 7368 1997 2037 4505 2031 1010 2588 2536 6642 1010 7944 2037 19799 2046 2162 1010 2030 2506 2068 1999 2009 1010 10043 2000 2037 21970 2015 1010 1998 2823 10043 2000 1996 2613 5426 1997 1996 2110 1012 1999 2008 13432 5998 2005 19113 2090 1996 6538 3506 1997 5118 1998 15477 1010 2029 2061 2146 2921 2885 1999 1037 8457 1010 2009 2003 2092 2124 2008 1996 3424 15069 3111 1997 1996 2394 2114 1996 2413 1010 2117 2075 1996 16290 1010 2030 2738 1996 10927 17599 1010 1997 1037 5440 3003 1010 2184 4013 24301 1996 2162 3458 1996 6537 4417 2041 2011 2614 3343 1010 1998 2005 1037 6196 2051 1999 4559 2000 1996 5328 1997 1996 2457 1012 1996 5233 1997 2122 2048 2197 1011 3855 3741 2031 1999 1037 2307 5468 4961 2041 1997 3293 16852 1010 1011 1011 1996 4792 1997 10514 9397 5802 3436 1998 1996 3571 1997 2108 10514 9397 5802 3064 1010 2593 1999 3327 5628 1997 4026 2030 1999 1996 2236 12637 1997 3119 1998 9163 1012 2013 2023 12654 1997 2054 2038 2579 2173 1999 2060 3032 1010 3005 8146 2031 15356 1996 7205 14062 2000 2256 2219 1010 2054 3114 2064 2057 2031 2000 9530 20740 1999 2216 7065 28077 2029 2052 23199 2149 2046 2019 17626 1997 3521 1998 11601 4818 3012 2090 102\n",
      "[NeMo I 2024-03-06 15:10:36 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-06 15:10:36 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-03-06 15:10:36 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2024-03-06 15:10:36 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-03-06 15:10:36 text_classification_dataset:239] example 1: ['They', 'would', ',', 'at', 'the', 'same', 'time', ',', 'be', 'necessitated', 'to', 'strengthen', 'the', 'executive', 'arm', 'of', 'government', ',', 'in', 'doing', 'which', 'their', 'constitutions', 'would', 'acquire', 'a', 'progressive', 'direction', 'toward', 'monarchy', '.It', 'is', 'of', 'the', 'nature', 'of', 'war', 'to', 'increase', 'the', 'executive', 'at', 'the', 'expense', 'of', 'the', 'legislative', 'authority', '.The', 'expedients', 'which', 'have', 'been', 'mentioned', 'would', 'soon', 'give', 'the', 'States', 'or', 'confederacies', 'that', 'made', 'use', 'of', 'them', 'a', 'superiority', 'over', 'their', 'neighbors', '.Small', 'states', ',', 'or', 'states', 'of', 'less', 'natural', 'strength', ',', 'under', 'vigorous', 'governments', ',', 'and', 'with', 'the', 'assistance', 'of', 'disciplined', 'armies', ',', 'have', 'often', 'triumphed', 'over', 'large', 'states', ',', 'or', 'states', 'of', 'greater', 'natural', 'strength', ',', 'which', 'have', 'been', 'destitute', 'of', 'these', 'advantages', '.Neither', 'the', 'pride', 'nor', 'the', 'safety', 'of', 'the', 'more', 'important', 'States', 'or', 'confederacies', 'would', 'permit', 'them', 'long', 'to', 'submit', 'to', 'this', 'mortifying', 'and', 'adventitious', 'superiority', '.They', 'would', 'quickly', 'resort', 'to', 'means', 'similar', 'to', 'those', 'by', 'which', 'it', 'had', 'been', 'effected', ',', 'to', 'reinstate', 'themselves', 'in', 'their', 'lost', 'pre-eminence', '.Thus', ',', 'we', 'should', ',', 'in', 'a', 'little', 'time', ',', 'see', 'established', 'in', 'every', 'part', 'of', 'this', 'country', 'the', 'same', 'engines', 'of', 'despotism', 'which', 'have', 'been', 'the', 'scourge', 'of', 'the', 'Old', 'World', '.This', ',', 'at', 'least', ',', 'would', 'be', 'the', 'natural', 'course', 'of', 'things', ';', 'and', 'our', 'reasonings', 'will', 'be', 'the', 'more', 'likely', 'to', 'be', 'just', ',', 'in', 'proportion', 'as', 'they', 'are', 'accommodated', 'to', 'this', 'standard', '.These', 'are', 'not', 'vague', 'inferences', 'drawn', 'from', 'supposed', 'or', 'speculative', 'defects', 'in', 'a', 'Constitution', ',', 'the', 'whole', 'power', 'of', 'which', 'is', 'lodged', 'in', 'the', 'hands', 'of', 'a', 'people', ',', 'or', 'their', 'representatives', 'and', 'delegates', ',', 'but', 'they', 'are', 'solid', 'conclusions', ',', 'drawn', 'from', 'the', 'natural', 'and', 'necessary', 'progress', 'of', 'human', 'affairs', '.']\n",
      "[NeMo I 2024-03-06 15:10:36 text_classification_dataset:240] subtokens: [CLS] they would , at the same time , be necessitated to strengthen the executive arm of government , in doing which their constitution ##s would acquire a progressive direction toward monarchy . it is of the nature of war to increase the executive at the expense of the legislative authority . the ex ##ped ##ient ##s which have been mentioned would soon give the states or con ##fed ##era ##cies that made use of them a superiority over their neighbors . small states , or states of less natural strength , under vigorous governments , and with the assistance of disciplined armies , have often triumph ##ed over large states , or states of greater natural strength , which have been des ##ti ##tute of these advantages . neither the pride nor the safety of the more important states or con ##fed ##era ##cies would permit them long to submit to this mort ##ifying and advent ##iti ##ous superiority . they would quickly resort to means similar to those by which it had been effect ##ed , to reins ##tate themselves in their lost pre - emi ##nen ##ce . thus , we should , in a little time , see established in every part of this country the same engines of des ##pot ##ism which have been the sc ##our ##ge of the old world . this , at least , would be the natural course of things ; and our reasoning ##s will be the more likely to be just , in proportion [SEP]\n",
      "[NeMo I 2024-03-06 15:10:36 text_classification_dataset:241] input_ids: 101 2027 2052 1010 2012 1996 2168 2051 1010 2022 29611 2000 12919 1996 3237 2849 1997 2231 1010 1999 2725 2029 2037 4552 2015 2052 9878 1037 6555 3257 2646 12078 1012 2009 2003 1997 1996 3267 1997 2162 2000 3623 1996 3237 2012 1996 10961 1997 1996 4884 3691 1012 1996 4654 5669 11638 2015 2029 2031 2042 3855 2052 2574 2507 1996 2163 2030 9530 25031 6906 9243 2008 2081 2224 1997 2068 1037 19113 2058 2037 10638 1012 2235 2163 1010 2030 2163 1997 2625 3019 3997 1010 2104 21813 6867 1010 1998 2007 1996 5375 1997 28675 8749 1010 2031 2411 10911 2098 2058 2312 2163 1010 2030 2163 1997 3618 3019 3997 1010 2029 2031 2042 4078 3775 24518 1997 2122 12637 1012 4445 1996 6620 4496 1996 3808 1997 1996 2062 2590 2163 2030 9530 25031 6906 9243 2052 9146 2068 2146 2000 12040 2000 2023 22294 11787 1998 13896 25090 3560 19113 1012 2027 2052 2855 7001 2000 2965 2714 2000 2216 2011 2029 2009 2018 2042 3466 2098 1010 2000 19222 12259 3209 1999 2037 2439 3653 1011 12495 10224 3401 1012 2947 1010 2057 2323 1010 1999 1037 2210 2051 1010 2156 2511 1999 2296 2112 1997 2023 2406 1996 2168 5209 1997 4078 11008 2964 2029 2031 2042 1996 8040 8162 3351 1997 1996 2214 2088 1012 2023 1010 2012 2560 1010 2052 2022 1996 3019 2607 1997 2477 1025 1998 2256 13384 2015 2097 2022 1996 2062 3497 2000 2022 2074 1010 1999 10817 102\n",
      "[NeMo I 2024-03-06 15:10:36 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-06 15:10:36 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-03-06 15:10:36 text_classification_dataset:244] label: 0\n",
      "[NeMo W 2024-03-06 15:10:38 text_classification_dataset:250] Found 115 out of 115 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo I 2024-03-06 15:10:38 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-03-06 15:10:38 data_preprocessing:301] Min: 257 |                  Max: 257 |                  Mean: 257.0 |                  Median: 257.0\n",
      "[NeMo I 2024-03-06 15:10:38 data_preprocessing:307] 75 percentile: 257.00\n",
      "[NeMo I 2024-03-06 15:10:38 data_preprocessing:308] 99 percentile: 257.00\n",
      "[NeMo I 2024-03-06 15:10:38 text_classification_model:216] Dataloader config or file_path for the test is missing, so no data loader for test is created!\n",
      "[NeMo W 2024-03-06 15:10:38 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "[NeMo W 2024-03-06 15:10:38 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertEncoder: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2024-03-06 15:10:40 text_classification_with_bert:118] ===========================================================================================\n",
      "[NeMo I 2024-03-06 15:10:40 text_classification_with_bert:119] Starting training...\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2024-03-06 15:10:40 modelPT:748] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.999]\n",
      "        eps: 1e-08\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2024-03-06 15:10:40 lr_scheduler:617] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7fd7a6f85940>\" \n",
      "    will be used during training (effective maximum steps = 80) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 80\n",
      "    )\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "\n",
      "  | Name                  | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | loss                  | CrossEntropyLoss     | 0     \n",
      "1 | bert_model            | BertEncoder          | 109 M \n",
      "2 | classifier            | SequenceClassifier   | 592 K \n",
      "3 | classification_report | ClassificationReport | 0     \n",
      "---------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.297   Total estimated model params size (MB)\n",
      "Epoch 0:  80%|▊| 8/10 [00:04<00:01,  1.80it/s, loss=0.597, v_num=0-27, lr=7.78e-\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 10/10 [00:04<00:00,  2.14it/s, loss=0.597, v_num=0-27, lr=7.78e\u001b[A\n",
      "Validating: 100%|█████████████████████████████████| 2/2 [00:00<00:00,  5.86it/s]\u001b[A[NeMo I 2024-03-06 15:10:46 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             77.39     100.00      87.25         89\n",
      "    label_id: 1                                              0.00       0.00       0.00         26\n",
      "    -------------------\n",
      "    micro avg                                               77.39      77.39      77.39        115\n",
      "    macro avg                                               38.70      50.00      43.63        115\n",
      "    weighted avg                                            59.89      77.39      67.53        115\n",
      "    \n",
      "Epoch 0: 100%|█| 10/10 [00:04<00:00,  2.07it/s, loss=0.597, v_num=0-27, lr=8.89e\n",
      "                                                                                \u001b[AEpoch 0, global step 7: val_loss reached 0.57149 (best 0.57149), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-03-06_15-10-27/checkpoints/TextClassification--val_loss=0.57-epoch=0.ckpt\" as top 3\n",
      "Epoch 1:  80%|▊| 8/10 [00:03<00:00,  2.04it/s, loss=0.593, v_num=0-27, lr=9.17e-\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 10/10 [00:04<00:00,  2.40it/s, loss=0.593, v_num=0-27, lr=9.17e\u001b[A\n",
      "Validating: 100%|█████████████████████████████████| 2/2 [00:00<00:00,  5.83it/s]\u001b[A[NeMo I 2024-03-06 15:10:54 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             77.39     100.00      87.25         89\n",
      "    label_id: 1                                              0.00       0.00       0.00         26\n",
      "    -------------------\n",
      "    micro avg                                               77.39      77.39      77.39        115\n",
      "    macro avg                                               38.70      50.00      43.63        115\n",
      "    weighted avg                                            59.89      77.39      67.53        115\n",
      "    \n",
      "Epoch 1: 100%|█| 10/10 [00:04<00:00,  2.31it/s, loss=0.593, v_num=0-27, lr=9.03e\n",
      "                                                                                \u001b[AEpoch 1, global step 15: val_loss reached 0.57344 (best 0.57149), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-03-06_15-10-27/checkpoints/TextClassification--val_loss=0.57-epoch=1.ckpt\" as top 3\n",
      "Epoch 2:  80%|▊| 8/10 [00:03<00:00,  2.04it/s, loss=0.56, v_num=0-27, lr=8.06e-5\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 10/10 [00:04<00:00,  2.40it/s, loss=0.56, v_num=0-27, lr=8.06e-\u001b[A\n",
      "Validating: 100%|█████████████████████████████████| 2/2 [00:00<00:00,  5.74it/s]\u001b[A[NeMo I 2024-03-06 15:11:02 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             78.76     100.00      88.12         89\n",
      "    label_id: 1                                            100.00       7.69      14.29         26\n",
      "    -------------------\n",
      "    micro avg                                               79.13      79.13      79.13        115\n",
      "    macro avg                                               89.38      53.85      51.20        115\n",
      "    weighted avg                                            83.56      79.13      71.43        115\n",
      "    \n",
      "Epoch 2: 100%|█| 10/10 [00:04<00:00,  2.31it/s, loss=0.56, v_num=0-27, lr=7.92e-\n",
      "                                                                                \u001b[AEpoch 2, global step 23: val_loss reached 0.52445 (best 0.52445), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-03-06_15-10-27/checkpoints/TextClassification--val_loss=0.52-epoch=2.ckpt\" as top 3\n",
      "Epoch 3:  80%|▊| 8/10 [00:03<00:00,  2.03it/s, loss=0.541, v_num=0-27, lr=6.94e-\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 10/10 [00:04<00:00,  2.40it/s, loss=0.541, v_num=0-27, lr=6.94e\u001b[A\n",
      "Validating: 100%|█████████████████████████████████| 2/2 [00:00<00:00,  5.84it/s]\u001b[A[NeMo I 2024-03-06 15:11:11 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             87.64      87.64      87.64         89\n",
      "    label_id: 1                                             57.69      57.69      57.69         26\n",
      "    -------------------\n",
      "    micro avg                                               80.87      80.87      80.87        115\n",
      "    macro avg                                               72.67      72.67      72.67        115\n",
      "    weighted avg                                            80.87      80.87      80.87        115\n",
      "    \n",
      "Epoch 3: 100%|█| 10/10 [00:04<00:00,  2.31it/s, loss=0.541, v_num=0-27, lr=6.81e\n",
      "                                                                                \u001b[AEpoch 3, global step 31: val_loss reached 0.53990 (best 0.52445), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-03-06_15-10-27/checkpoints/TextClassification--val_loss=0.54-epoch=3.ckpt\" as top 3\n",
      "Epoch 4:  80%|▊| 8/10 [00:03<00:00,  2.03it/s, loss=0.5, v_num=0-27, lr=5.83e-5,\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 10/10 [00:04<00:00,  2.39it/s, loss=0.5, v_num=0-27, lr=5.83e-5\u001b[A\n",
      "Validating: 100%|█████████████████████████████████| 2/2 [00:00<00:00,  5.80it/s]\u001b[A[NeMo I 2024-03-06 15:11:19 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             84.00      94.38      88.89         89\n",
      "    label_id: 1                                             66.67      38.46      48.78         26\n",
      "    -------------------\n",
      "    micro avg                                               81.74      81.74      81.74        115\n",
      "    macro avg                                               75.33      66.42      68.83        115\n",
      "    weighted avg                                            80.08      81.74      79.82        115\n",
      "    \n",
      "Epoch 4: 100%|█| 10/10 [00:04<00:00,  2.30it/s, loss=0.5, v_num=0-27, lr=5.69e-5\n",
      "                                                                                \u001b[AEpoch 4, global step 39: val_loss reached 0.42953 (best 0.42953), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-03-06_15-10-27/checkpoints/TextClassification--val_loss=0.43-epoch=4.ckpt\" as top 3\n",
      "Epoch 5:  80%|▊| 8/10 [00:03<00:00,  2.03it/s, loss=0.42, v_num=0-27, lr=4.72e-5\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 10/10 [00:04<00:00,  2.39it/s, loss=0.42, v_num=0-27, lr=4.72e-\u001b[A\n",
      "Validating: 100%|█████████████████████████████████| 2/2 [00:00<00:00,  5.75it/s]\u001b[A[NeMo I 2024-03-06 15:11:28 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             86.46      93.26      89.73         89\n",
      "    label_id: 1                                             68.42      50.00      57.78         26\n",
      "    -------------------\n",
      "    micro avg                                               83.48      83.48      83.48        115\n",
      "    macro avg                                               77.44      71.63      73.75        115\n",
      "    weighted avg                                            82.38      83.48      82.51        115\n",
      "    \n",
      "Epoch 5: 100%|█| 10/10 [00:04<00:00,  2.30it/s, loss=0.42, v_num=0-27, lr=4.58e-\n",
      "                                                                                \u001b[AEpoch 5, global step 47: val_loss reached 0.37257 (best 0.37257), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-03-06_15-10-27/checkpoints/TextClassification--val_loss=0.37-epoch=5.ckpt\" as top 3\n",
      "Epoch 6:  80%|▊| 8/10 [00:03<00:00,  2.03it/s, loss=0.286, v_num=0-27, lr=3.61e-\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 10/10 [00:04<00:00,  2.40it/s, loss=0.286, v_num=0-27, lr=3.61e\u001b[A\n",
      "Validating: 100%|█████████████████████████████████| 2/2 [00:00<00:00,  5.86it/s]\u001b[A[NeMo I 2024-03-06 15:11:36 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             86.14      97.75      91.58         89\n",
      "    label_id: 1                                             85.71      46.15      60.00         26\n",
      "    -------------------\n",
      "    micro avg                                               86.09      86.09      86.09        115\n",
      "    macro avg                                               85.93      71.95      75.79        115\n",
      "    weighted avg                                            86.04      86.09      84.44        115\n",
      "    \n",
      "Epoch 6: 100%|█| 10/10 [00:04<00:00,  2.31it/s, loss=0.286, v_num=0-27, lr=3.47e\n",
      "                                                                                \u001b[AEpoch 6, global step 55: val_loss reached 0.42354 (best 0.37257), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-03-06_15-10-27/checkpoints/TextClassification--val_loss=0.42-epoch=6.ckpt\" as top 3\n",
      "Epoch 7:  80%|▊| 8/10 [00:03<00:00,  2.03it/s, loss=0.193, v_num=0-27, lr=2.5e-5\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 10/10 [00:04<00:00,  2.40it/s, loss=0.193, v_num=0-27, lr=2.5e-\u001b[A\n",
      "Validating: 100%|█████████████████████████████████| 2/2 [00:00<00:00,  5.85it/s]\u001b[A[NeMo I 2024-03-06 15:11:44 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             89.77      88.76      89.27         89\n",
      "    label_id: 1                                             62.96      65.38      64.15         26\n",
      "    -------------------\n",
      "    micro avg                                               83.48      83.48      83.48        115\n",
      "    macro avg                                               76.37      77.07      76.71        115\n",
      "    weighted avg                                            83.71      83.48      83.59        115\n",
      "    \n",
      "Epoch 7: 100%|█| 10/10 [00:04<00:00,  2.31it/s, loss=0.193, v_num=0-27, lr=2.36e\n",
      "                                                                                \u001b[AEpoch 7, global step 63: val_loss reached 0.35701 (best 0.35701), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-03-06_15-10-27/checkpoints/TextClassification--val_loss=0.36-epoch=7.ckpt\" as top 3\n",
      "Epoch 8:  80%|▊| 8/10 [00:03<00:00,  2.03it/s, loss=0.117, v_num=0-27, lr=1.39e-\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 10/10 [00:04<00:00,  2.39it/s, loss=0.117, v_num=0-27, lr=1.39e\u001b[A\n",
      "Validating: 100%|█████████████████████████████████| 2/2 [00:00<00:00,  5.73it/s]\u001b[A[NeMo I 2024-03-06 15:11:53 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             90.00      91.01      90.50         89\n",
      "    label_id: 1                                             68.00      65.38      66.67         26\n",
      "    -------------------\n",
      "    micro avg                                               85.22      85.22      85.22        115\n",
      "    macro avg                                               79.00      78.20      78.58        115\n",
      "    weighted avg                                            85.03      85.22      85.11        115\n",
      "    \n",
      "Epoch 8: 100%|█| 10/10 [00:04<00:00,  2.30it/s, loss=0.117, v_num=0-27, lr=1.25e\n",
      "                                                                                \u001b[AEpoch 8, global step 71: val_loss reached 0.34771 (best 0.34771), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-03-06_15-10-27/checkpoints/TextClassification--val_loss=0.35-epoch=8.ckpt\" as top 3\n",
      "Epoch 9:  80%|▊| 8/10 [00:03<00:00,  2.02it/s, loss=0.0722, v_num=0-27, lr=2.78e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 10/10 [00:04<00:00,  2.38it/s, loss=0.0722, v_num=0-27, lr=2.78\u001b[A\n",
      "Validating: 100%|█████████████████████████████████| 2/2 [00:00<00:00,  5.75it/s]\u001b[A[NeMo I 2024-03-06 15:12:01 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             87.23      92.13      89.62         89\n",
      "    label_id: 1                                             66.67      53.85      59.57         26\n",
      "    -------------------\n",
      "    micro avg                                               83.48      83.48      83.48        115\n",
      "    macro avg                                               76.95      72.99      74.60        115\n",
      "    weighted avg                                            82.58      83.48      82.83        115\n",
      "    \n",
      "Epoch 9: 100%|█| 10/10 [00:04<00:00,  2.29it/s, loss=0.0722, v_num=0-27, lr=1.39\n",
      "                                                                                \u001b[AEpoch 9, global step 79: val_loss reached 0.36380 (best 0.34771), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-03-06_15-10-27/checkpoints/TextClassification--val_loss=0.36-epoch=9.ckpt\" as top 3\n",
      "Saving latest checkpoint...\n",
      "Epoch 9: 100%|█| 10/10 [00:08<00:00,  1.18it/s, loss=0.0722, v_num=0-27, lr=1.39\n",
      "[NeMo W 2024-03-06 15:12:05 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:308: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      conf.update_node(conf_path, item.path)\n",
      "    \n",
      "[NeMo I 2024-03-06 15:12:28 text_classification_with_bert:121] Training finished!\n",
      "[NeMo I 2024-03-06 15:12:28 text_classification_with_bert:122] ===========================================================================================\n",
      "[NeMo I 2024-03-06 15:12:55 text_classification_with_bert:127] Model is saved into `.nemo` file: text_classification_model.nemo\n",
      "CPU times: user 1.19 s, sys: 733 ms, total: 1.92 s\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run the training script, overriding the config values in the command line\n",
    "TC_DIR = \"/dli/task/nemo/examples/nlp/text_classification\"\n",
    "\n",
    "\n",
    "!python $TC_DIR/text_classification_with_bert.py \\\n",
    "        model.dataset.num_classes=$NUM_CLASSES \\\n",
    "        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "        model.train_ds.file_path=$PATH_TO_TRAIN_FILE \\\n",
    "        model.validation_ds.file_path=$PATH_TO_VAL_FILE \\\n",
    "        model.infer_samples=[] \\\n",
    "        trainer.max_epochs=$MAX_EPOCHS \\\n",
    "        model.train_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.validation_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.language_model.pretrained_model_name=$PRETRAINED_MODEL_NAME \\\n",
    "        trainer.amp_level=$AMP_LEVEL \\\n",
    "        trainer.precision=$PRECISION \\\n",
    "        model.optim.lr=$LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "UJQzX3rPhMF2"
   },
   "outputs": [],
   "source": [
    "# Run to save for assessment- DO NOT CHANGE\n",
    "cmd_log = os.path.join(os.path.dirname(os.path.dirname(get_latest_model())),'cmd-args.log')\n",
    "lightning_logs = os.path.join(os.path.dirname(os.path.dirname(get_latest_model())),'lightning_logs.txt')\n",
    "\n",
    "with open(cmd_log, \"r\") as f:\n",
    "    cmd = f.read()\n",
    "    cmd_list = cmd.split()\n",
    "with open(\"my_assessment/step4.json\", \"w\") as outfile: \n",
    "    json.dump(cmd_list, outfile) \n",
    "    \n",
    "with open(lightning_logs, \"r\") as f:\n",
    "    log = f.readlines()\n",
    "with open(\"my_assessment/step4_lightning.json\", \"w\") as outfile:\n",
    "    json.dump(log, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UqFWUsxhMF2"
   },
   "source": [
    "---\n",
    "# 5단계: 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1jZfWUqhMF2"
   },
   "source": [
    "### 추론 실행하기 (평가됨)\n",
    "추론 블록을 실행하고 결과를 확인하고 저장합니다. (노트: 여기는 추가로 고칠 부분이 없습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "mlnzT13DhMF2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2024-03-06 15:01:01 modelPT:137] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    file_path: /dli/task/data/federalist_papers_HM/train_nemo_format.tsv\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2024-03-06 15:01:01 modelPT:144] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    file_path: /dli/task/data/federalist_papers_HM/dev_nemo_format.tsv\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2024-03-06 15:01:01 modelPT:151] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    file_path: null\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2024-03-06 15:01:01 modelPT:1198] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2024-03-06 15:01:01 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "[NeMo W 2024-03-06 15:01:01 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-06 15:01:04 modelPT:434] Model TextClassificationModel was successfully restored from nemo_experiments/TextClassification/2024-03-06_14-57-24/checkpoints/TextClassification.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-06 15:01:04 text_classification_dataset:250] Found 7 out of 7 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2024-03-06 15:01:05 text_classification_dataset:250] Found 4 out of 4 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2024-03-06 15:01:05 text_classification_dataset:250] Found 7 out of 8 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2024-03-06 15:01:06 text_classification_dataset:250] Found 7 out of 7 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2024-03-06 15:01:06 text_classification_dataset:250] Found 9 out of 9 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2024-03-06 15:01:06 text_classification_dataset:250] Found 8 out of 8 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2024-03-06 15:01:07 text_classification_dataset:250] Found 8 out of 8 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2024-03-06 15:01:07 text_classification_dataset:250] Found 6 out of 6 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2024-03-06 15:01:07 text_classification_dataset:250] Found 9 out of 9 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2024-03-06 15:01:08 text_classification_dataset:250] Found 22 out of 22 sentences with more than 256 subtokens. Truncated long sentences from the end.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 0, 1, 1, 0, 0], [0, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1], [0, 1, 1, 0, 1, 0, 1], [0, 1, 1, 0, 1, 1, 1, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1], [1, 1, 0, 0, 0, 0, 1, 0], [0, 1, 1, 0, 1, 0], [1, 1, 1, 0, 0, 1, 0, 1, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Run inference for assessment -  - DO NOT CHANGE\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "\n",
    "# Instantiate the model by restoring from the latest .nemo checkpoint\n",
    "model = nemo_nlp.models.TextClassificationModel.restore_from(get_latest_model())\n",
    "\n",
    "# Find the latest model path\n",
    "DATA_DIR = '/dli/task/data/federalist_papers_HM'\n",
    "\n",
    "test_files = [\n",
    "    'test49.tsv',\n",
    "    'test50.tsv',\n",
    "    'test51.tsv',\n",
    "    'test52.tsv',\n",
    "    'test53.tsv',\n",
    "    'test54.tsv', \n",
    "    'test55.tsv',\n",
    "    'test56.tsv',\n",
    "    'test57.tsv',\n",
    "    'test62.tsv',\n",
    "]\n",
    "results = []\n",
    "for test_file in test_files:\n",
    "    # get as list and remove header row\n",
    "    filepath = os.path.join(DATA_DIR, test_file)\n",
    "    with open(filepath, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    del lines[0]\n",
    "    \n",
    "    results.append(model.classifytext(lines, batch_size = 1, max_seq_length = 256))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "f70jcxRDhMF3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAMILTON\n",
      "MADISON\n",
      "MADISON\n",
      "MADISON\n",
      "MADISON\n",
      "MADISON\n",
      "HAMILTON\n",
      "MADISON\n",
      "MADISON\n",
      "HAMILTON\n"
     ]
    }
   ],
   "source": [
    "# Run to save for assessment- DO NOT CHANGE\n",
    "author = []\n",
    "for result in results:\n",
    "    avg_result = sum(result) / len(result)\n",
    "    if avg_result < 0.5:\n",
    "        author.append(\"HAMILTON\")\n",
    "        print(\"HAMILTON\")\n",
    "    else:\n",
    "        author.append(\"MADISON\")\n",
    "        print(\"MADISON\")\n",
    "        \n",
    "with open(\"my_assessment/step5.json\", \"w\") as outfile: \n",
    "    json.dump(author, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISARdk0HhMF3"
   },
   "source": [
    "# 6단계: 평가 제출\n",
    "결과가 어땠습니까?  [위키디피아 문서](https://en.wikipedia.org/wiki/The_Federalist_Papers)에 따르면 과거의 전문가들은 Madison이 논쟁이 되고 있는 모든 논문의 저자라고 믿었지만 최근 분석에서는 일부 공저의 가능성도 제시됩니다.  갖고 있는 도구를 사용해 \"모두 MADISON\"이라는 답을 얻을 수도 있습니다.  원한다면 계속 시도해 봐도 되지만 **특정 결과는 평가에 합격하는 데 필요하지 *않습니다***.\n",
    "\n",
    "코드를 올바르게 작성했고 트레이닝 및 추론이 올바르게 작동하고 있다고 확신한다면 다음과 같이 오토그레이더(autograder)에 프로젝트를 제출할 수 있습니다.\n",
    "\n",
    "1. GPU 시작 페이지로 돌아가서 체크 표시를 클릭해 평가를 실행합니다.\n",
    "\n",
    "<img src=\"../images/assessment_checkmark.png\" width=600>\n",
    "\n",
    "2. 잘 하셨습니다!  합격했으면 합격을 알리는 팝업 창이 표시되고 진도에 점수가 추가됩니다.  그렇지 않은 경우 팝업 창에 피드백이 표시됩니다. \n",
    "\n",
    "<img src=\"../images/assessment_pass_popup.png\" width=600>\n",
    "\n",
    "과정 진도 탭에서 항상 평가 진행 상황을 확인할 수 있습니다.  코딩 평가의 부분 값은 여기에 표시되지 않으며, 0점 또는 70점으로 표시됩니다.  동일한 과정 페이지의 트랜스포머 및 배포에서 질문을 완료하면 최종 인증서를 받을 수 있는 자격이 부여됩니다!\n",
    "\n",
    "<img src=\"../images/progress.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlQ_LnwlhMF3"
   },
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"../images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assessment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
